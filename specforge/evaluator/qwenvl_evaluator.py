import torch
from typing import List, Optional, Tuple

from specforge.core.eagle3 import Eagle3Model
from specforge.modeling.draft.llama3_eagle import _make_causal_mask
from transformers.cache_utils import DynamicCache

class QwenVLEagle3Evaluator():
    def __init__(
        self,
        eagle3_model: Eagle3Model,
        draft_length: int = 7,
    ):
        """Evaluator for Qwen-VL Eagle3 model performance assessment.
        
        Args:
            eagle3_model: The Eagle3 model instance containing draft and target models
            draft_length: Number of tokens to generate in draft phase
        """
        self.eagle3_model = eagle3_model
        self.draft_model = eagle3_model.draft_model
        self.target_model = eagle3_model.target_model
        self.draft_length = draft_length
        self.attention_backend = eagle3_model.attention_backend
    
    def draft_token_valid(
        self,
        draft_ids: torch.Tensor,
        target_ids: torch.Tensor
    ) -> torch.Tensor:
        """Calculate the average acceptance length between draft and target sequences.
        
        Args:
            draft_ids: Token IDs generated by draft model [batch_size, seq_len]
            target_ids: Ground truth token IDs [batch_size, seq_len]
            
        Returns:
            Average number of consecutive matching tokens from the beginning
        """
        bsz = draft_ids.shape[0]
        acc_len = torch.zeros(bsz, dtype=torch.long, device=draft_ids.device)
        
        # Check token-by-token matching from the start
        for i in range(self.draft_length):
            mask = (draft_ids[:, i] == target_ids[:, i])
            still_valid = (acc_len == i)  # Only increment if previous tokens matched
            can_increment = mask & still_valid
            acc_len[can_increment] += 1
    
        return acc_len.float().mean()

    def draft_generate(
        self,
        input_ids: torch.Tensor,
        hidden_states: torch.Tensor,
        cache_hidden: List[List],
        past_key_values: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        pixel_values: Optional[torch.Tensor] = None,
        image_grid_thw: Optional[torch.Tensor] = None,
        seq_length_with_past: int = 0,
        **kwargs,
    ) -> torch.Tensor:
        """Generate candidate token sequence using draft model (autoregressive decoding).
        
        Performs autoregressive decoding with the draft model to generate candidate token 
        sequences for speculative decoding.
        
        Args:
            input_ids: Input token IDs [batch_size, seq_len]
            hidden_states: Hidden states from target model for draft initialization
            cache_hidden: KV cache management list for storing past key-values
            past_key_values: Past key-value cache for incremental decoding
            pixel_values: Image pixel values for vision-language tasks
            image_grid_thw: Spatiotemporal dimensions of image grids
            seq_length_with_past: Total sequence length including history for position encoding
            **kwargs: Additional optional arguments
            
        Returns:
            Generated candidate token sequence [batch_size, draft_length-1]
            
        Notes:
            - Uses greedy decoding strategy for candidate generation
            - Supports two attention backends: sdpa and flex_attention
            - Uses visual embedding for first decode, text embedding for subsequent steps
            - Maps generated tokens to target model vocabulary via d2t mapping
        """
        
        # Initialize generation sequence by cloning input IDs
        generated_ids = input_ids.clone()
        # Extract current step hidden states (excluding history)
        hidden_states = hidden_states[:, seq_length_with_past:, :]
        # Get current token IDs to process
        current_ids = input_ids[:, seq_length_with_past:]
        bsz, seq_len = current_ids.shape
        
        # Initialize cache management parameters based on attention backend
        if self.attention_backend == "sdpa":
            lck = len(cache_hidden[0])  # Current cache length
        elif self.attention_backend == "flex_attention":
            past_seen_tokens = seq_len + (
                past_key_values.get_seq_length() if past_key_values is not None else 0
            )

        # Autoregressive decoding loop: generate draft_length-1 tokens
        for step in range(self.draft_length - 1):
            bsz, seq_len = current_ids.shape
            
            # Step 1: Prepare input embeddings
            if seq_length_with_past == 0:
                # First decode (prefill phase): use visual embedding if available
                inputs_embeds = self.eagle3_model._get_input_embeds(
                    current_ids, pixel_values, image_grid_thw
                ).to(hidden_states.dtype)
            else:
                # Subsequent decodes: use text embedding only
                inputs_embeds = self.draft_model.embed_input_ids(current_ids).to(hidden_states.dtype)

            # Step 2: Create causal attention mask
            if self.attention_backend == "sdpa":
                causal_mask = _make_causal_mask(
                    input_ids_shape=current_ids.shape,
                    dtype=hidden_states.dtype,
                    device=hidden_states.device,
                    past_key_values_length=seq_length_with_past
                )
            elif self.attention_backend == "flex_attention":
                causal_mask = torch.ones(
                    (bsz, seq_len + seq_length_with_past),
                    dtype=torch.bool,
                    device=hidden_states.device,
                )
                
            # Step 3: Create position IDs for rotary encoding
            position_ids, _ = self.target_model.model.get_rope_index(
                generated_ids,
                image_grid_thw,
                None,
                second_per_grid_ts=None,
                attention_mask=None,
            )
            # Take only position IDs for current sequence
            position_ids = position_ids[:, :, -seq_len:]
            
            # Step 4: Forward pass through draft model backbone
            hidden_states_out = self.draft_model.backbone(
                input_embeds=inputs_embeds,
                hidden_states=hidden_states,
                attention_mask=causal_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                cache_hidden=cache_hidden,  # Enable KV cache management
                use_cache=True,
            )
            
            # Step 5: Update hidden states (take last token output)
            hidden_states = hidden_states_out[:, -1:, :]
            
            # Step 6: Compute logits and select next token
            logits = self.draft_model.compute_logits(hidden_states)
            next_token_logits = logits[:, -1, :]  # [batch_size, vocab_size]
            
            # Greedy decoding: select token with highest probability
            next_token_ids = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            # Map to target model vocabulary space
            next_token_ids = next_token_ids + self.draft_model.d2t[next_token_ids]
            
            # Step 7: Update generated sequence
            generated_ids = torch.cat([generated_ids, next_token_ids], dim=1)
            current_ids = next_token_ids
            seq_length_with_past += seq_len
        
        # Step 8: Clean up KV cache (trim to valid length)
        if self.attention_backend == "sdpa":
            cache_hidden[0] = cache_hidden[0][:lck + 1]
            cache_hidden[1] = cache_hidden[1][:lck + 1]
        elif self.attention_backend == "flex_attention":
            past_key_values.crop(past_seen_tokens)

        # Return generated candidate sequence (exclude input, keep draft portion only)
        return generated_ids[:, -(self.draft_length-1):]

    def evaluation(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        loss_mask: torch.Tensor,
        past_key_values: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        position_ids: Optional[torch.Tensor] = None,
        pixel_values: Optional[torch.Tensor] = None,
        image_grid_thw: Optional[torch.Tensor] = None,
    ) -> float:
        """Evaluate draft model performance by calculating average acceptance length.
        
        Simulates speculative decoding process to evaluate how many candidate tokens 
        generated by the draft model are accepted by the target sequence, measuring 
        draft model accuracy and acceleration potential.
        
        Args:
            input_ids: Input token IDs [batch_size, seq_len]
            attention_mask: Attention mask identifying valid token positions
            loss_mask: Loss calculation mask identifying tokens for loss computation
            past_key_values: Past key-value cache for incremental decoding
            position_ids: Position encoding IDs
            pixel_values: Image pixel values for multimodal tasks
            image_grid_thw: Spatiotemporal dimensions of image grids
            
        Returns:
            Average acceptance length indicating how many draft tokens are accepted by target
            
        Notes:
            - Currently supports only batch_size=1 evaluation
            - Uses sliding window approach over entire sequence
            - Calculates acceptance length by comparing draft tokens with target tokens
            - Supports two attention backends: sdpa and flex_attention
        """
        
        # Step 0: Data preparation and validation
        bsz, _ = input_ids.shape
        assert bsz == 1  # Currently only supports batch_size=1 evaluation
        
        # Step 1: Prepare hidden states using target model
        hidden_states, _, _, input_ids = self.eagle3_model._prepare_data(
            input_ids, attention_mask, loss_mask, pixel_values, image_grid_thw
        )
        # Project target model hidden states to draft model space
        hidden_states = self.draft_model.project_hidden_states(hidden_states)

        # Determine decoding start position (first token to predict)
        start_pos = torch.argmax(loss_mask.squeeze(-1), dim=1, keepdim=True)[-1].item()
        start_idx = 1  # Sliding window start index

        # Step 2: Initialize cache based on attention backend
        if self.attention_backend == "sdpa":
            cache_hidden = [[], []]  # First two for attention, rest for fp8 k_cache k_scale for indexer
            past_key_values = None
        elif self.attention_backend == "flex_attention":
            cache_hidden = None
            past_key_values = DynamicCache()

        seq_length_with_past = 0  # Processed sequence length (including history)

        # Step 3: Initialize evaluation metrics
        round_num = 0  # Evaluation round counter
        avg_accept_length = 0  # Cumulative acceptance length
        
        # Step 4: Sliding window evaluation loop
        # Iterate through entire sequence, moving draft_length tokens each time
        while start_pos + self.draft_length + start_idx - 1 < len(input_ids[0]):
            # Get target token sequence for current window
            target_ids = input_ids[:, start_pos + start_idx - 1:start_pos + self.draft_length + start_idx - 1]
            
            # Prepare hidden states and input IDs for current decoding
            cur_hidden_states = hidden_states[:, :start_pos + start_idx, :].clone()
            cur_input_ids = input_ids[:, :start_pos + start_idx].clone()

            # Step 5: Generate candidate token sequence using draft model
            draft_ids = self.draft_generate(
                input_ids=cur_input_ids,
                hidden_states=cur_hidden_states,
                cache_hidden=cache_hidden,
                past_key_values=past_key_values,
                pixel_values=pixel_values,
                image_grid_thw=image_grid_thw,
                seq_length_with_past=seq_length_with_past
            )
            
            # Step 6: Construct complete draft sequence (include last token from current input)
            draft_ids = torch.cat([cur_input_ids[:, -1:], draft_ids], dim=-1)
            
            # Update cached sequence length (draft_generate only caches input, not decoded draft tokens)
            seq_length_with_past = start_pos + start_idx
            
            # Step 7: Calculate acceptance length for current window
            accept_length = self.draft_token_valid(draft_ids, target_ids)
            avg_accept_length += accept_length
            round_num += 1
            
            # Step 8: Move sliding window
            start_idx += self.draft_length

        # Step 9: Calculate average acceptance length
        if round_num > 0:
            avg_accept_length /= round_num
        else:
            avg_accept_length = 0.0

        return avg_accept_length