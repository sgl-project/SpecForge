# Preproducing the draft model in the EAGLE3 paper

This documents shows how to reproduce the training process of EAGLE3 paper. The script is in `examples/run_llama3_eagle3_sgl_online.sh`. This documents is a walk through of the script and explains all the middle points.

## Step0. Prepare environment

```
uv venv --python 3.11
source .venv/bin/activate
cd PATH-TO-SpecForge
uv pip install -r requirements.txt
uv pip install -v .
```

After completing these steps, open a Python shell and run:
```python
import specforge
```
If the import succeeds without errors, Step 0 is complete.

## Step1. Prepare Model & Dataset

First, use these command to download the model and the dataset.
```
hf download meta-llama/Llama-3.1-8B-Instruct
hf download Aeala/ShareGPT_Vicuna_unfiltered --repo-type dataset
hf download HuggingFaceH4/ultrachat_200k --repo-type dataset

python scripts/prepare_data.py --dataset ultrachat --output_path /root/.cache/user_artifacts/Llama-3.1-8B-Instruct/dataset
python scripts/prepare_data.py --dataset sharegpt --output_path /root/.cache/user_artifacts/Llama-3.1-8B-Instruct/dataset

for i in {1..4}; do
    CUDA_VISIBLE_DEVICES=$i python3 -m sglang.launch_server \
        --model meta-llama/Llama-3.1-8B-Instruct \
        --cuda-graph-bs 1 2 4 8 16 32 64 128 256 512 \
        --dtype bfloat16 --mem-frac=0.8 --port $((30000 + i)) &
done

python scripts/generate_data_by_target.py \
    --model-name meta-llama/Llama-3.1-8B-Instruct \
    --raw-data-file /root/.cache/user_artifacts/Llama-3.1-8B-Instruct/dataset/sharegpt.jsonl \
    --output-dir /root/.cache/user_artifacts/Llama-3.1-8B-Instruct/dataset/sharegpt-llama-3.1-8b-instruct \
    --max-concurrency 512 \
    --num-per-shard 50000 \
    --server-address-port 127.0.0.1:30001 127.0.0.1:30002 127.0.0.1:30003 127.0.0.1:30004

python scripts/generate_data_by_target.py \
    --model-name meta-llama/Llama-3.1-8B-Instruct \
    --raw-data-file /root/.cache/user_artifacts/Llama-3.1-8B-Instruct/dataset/ultrachat.jsonl \
    --output-dir /root/.cache/user_artifacts/Llama-3.1-8B-Instruct/dataset/ultrachat-llama-3.1-8b-instruct \
    --max-concurrency 512 \
    --num-per-shard 50000 \
    --server-address-port 127.0.0.1:30001 127.0.0.1:30002 127.0.0.1:30003 127.0.0.1:30004
```

Alternatively, For `meta-llama/Llama-3.1-8B-Instruct`, you can use the dataset we generated: [zhuyksir/Ultrachat-Sharegpt-Llama3.1-8B](https://huggingface.co/datasets/zhuyksir/Ultrachat-Sharegpt-Llama3.1-8B).

Each row should have this structure:
```json
{
    "id": XXX,
    "conversations":[
        {"role": "system", "content": XXX},
        {"role": "user", "content": XXX},
        {"role": "assistant", "content": XXX},
        ...
    ]
}
```

Second, we need to pre-build the cache for training.

- During training, the text must be encoded into input IDs. These encoding steps can be performed before training begins. The resulting cache file will be saved under `$CACHE_DIR`.
- The script also selects the vocabulary with the top-k size.
- With the option `--view train-data`, you can inspect the dataset by index (e.g., index 1 or index 2 in the example below). This helps verify that the loss mask is generated correctly:
    - Green text indicates tokens where `loss_mask == 1`.
    - Red text indicates tokens where `loss_mask == 0 (typically user input and system prompt)`. Since the goal is to train the draft model only on the target model’s output, user text must be masked out. In other words, only tokens generated by the target model should contribute to the loss.

- You might see this warning. `WARNING: No assistant response spans found in the conversation text.`This occurs when, during data generation, an error causes a sample to contain only user inputs without any assistant responses. You can safely ignore this warning—the loss mask for such samples is set entirely to zero.
```shell
python scripts/build_eagle3_dataset_cache.py \
    --target-model-path $MODEL_PATH \
    --draft-model-config ./configs/llama3-8B-eagle3.json \
    --train-data-path $DATASET_PATH/sharegpt_ultrachat_train.jsonl \
    --eval-data-path $DATASET_PATH/sharegpt_ultrachat_test.jsonl \
    --cache-dir $CACHE_DIR \
    --chat-template $CHAT_TEMPLATE \
    --max-length $MAX_LENGTH \
    --view-train-data 1 2
```

## Step2. Start Training

Use the following script to train.

- set `total-steps=800000, learning-rate=5e-5` to align with [EAGLE official repo config](https://github.com/SafeAILab/EAGLE/blob/main/eagle/traineagle3/ds_config.json). Feel Free to change this settings to do your own experiments. `total-steps` and `warmup-ratio` decide the increasement curve of learning rate.

```shell
export NUM_GPUS=4
export OUTPUT_DIR=~/.cache/huggingface/Llama-3.1-8B-Instruct/dev_outputs/
CUDA_VISIBLE_DEVICES=4,5,6,7 torchrun \
    --standalone \
    --nproc_per_node $NUM_GPUS \
    scripts/train_eagle3_sgl_online.py \
    --target-model-path $MODEL_PATH \
    --model-path $MODEL_PATH \
    --draft-model-config ./configs/llama3-8B-eagle3.json \
    --train-data-path $DATASET_PATH/sharegpt_ultrachat_train.jsonl \
    --eval-data-path $DATASET_PATH/sharegpt_ultrachat_test.jsonl \
    --tp-size $NUM_GPUS \
    --output-dir $OUTPUT_DIR \
    --num-epochs 10 \
    --batch-size 1 \
    --learning-rate 5e-5 \
    --draft-attention-backend flex_attention \
    --max-length $MAX_LENGTH \
    --chat-template $CHAT_TEMPLATE \
    --cache-dir $CACHE_DIR \
    --mem-frac=0.4 \
    --total-steps=800000 \
    --dist-timeout=10 \
    --wandb-project llama3-8b-eagle3 \
    --wandb-name sgl-online \
    --report-to wandb
```

## Step3. benchmark

```shell
config_list=(
    "4,3,1,4"
    "4,7,10,60"
)
CUDA_VISIBLE_DEVICES=4,5,6,7 python3 bench_model_speedup.py \
    --model-path meta-llama/Llama-3.1-8B-Instruct \
    --speculative-draft-model-path zhuyksir/EAGLE3-gpt-oss-120b-bf16 \
    --port 20001 \
    --trust-remote-code \
    --mem-fraction-static 0.8 \
    --tp-size 4 \
    --config-list "${config_list[@]}" \
    --benchmark-list mtbench:80 gsm8k:200 humaneval:200 math500:200 \
    --output output.jsonl
```
