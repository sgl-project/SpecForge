import torch
from dataclasses import dataclass
from abc import ABC, abstractmethod
from typing import Tuple
from sgl_eagle.modeling.auto_model import AutoModelForCausalLM
from transformers import AutoTokenizer
from transformers.cache_utils import DynamicCache
from .tree import initialize_tree, tree_decoding, update_inference_inputs

@dataclass
class Usage:
    """
    The runtime stats during inference when using EagleRunner.

    Args:
        num_generated_tokens: The number of tokens generated by the eagle model.
        accepted_tokens: The number of tokens generated by the draft model and accepted by the base model.
    """
    num_generated_tokens: int
    accepted_tokens: int

    def __str__(self):
        return f"""
Usage:
    num_generated_tokens: {self.num_generated_tokens}
    accepted_tokens: {self.accepted_tokens}
"""

class EagleRunner:
    """
    This class is a simple runner to run inference with Eagle3. It serves as a simple verifier of the speedup of Eagle3. 
    In case you want to use more advanced features such kernel optimization and distributed inference, you should try SGLang.
    """

    def __init__(self, base_model_path, draft_model_path):
        # TODO: support loading from the sgl-ealge lib
        print(draft_model_path)
        self.base_model = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype=torch.bfloat16).cuda().eval()
        self.draft_model = AutoModelForCausalLM.from_pretrained(draft_model_path, torch_dtype=torch.bfloat16).cuda().eval()
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)

    @torch.inference_mode()
    def run(self, prompt: str, enable_eagle3: bool = True, max_new_tokens: int = 512) -> str:
        """
        Run the eagle model with the given prompt and sampling parameters. 

        Args:
            prompt (str): The prompt to run the eagle model with.
            enable_eagle (bool): Whether to enable eagle.

        Returns:
            Tuple[str, Usage]: The generated text and the usage stats.
        """
        # apply chat template if any
        messages = [
            {"role": "user", "content": prompt},
        ]
        input_ids = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
        input_ids = input_ids.to(self.base_model.device)
        
        if enable_eagle3:
            gen_func = self._eagle_generate
        else:
            gen_func = self._naive_generate

        output = []
        for output_id in gen_func(input_ids, max_new_tokens):
            output.append(output_id)
        output = torch.cat(output, dim=-1)
        output = self.tokenizer.batch_decode(output, skip_special_tokens=True)

        if enable_eagle3:
            usage = Usage(
                num_generated_tokens=output.shape[-1],
                accepted_tokens=0,
            )
        else:
            usage = Usage(num_generated_tokens=max_new_tokens, accepted_tokens=0)
        return output, usage
        
    def _naive_generate(self, input_ids: str, max_new_tokens: int):
        input_ids = input_ids.clone()

        # Initialize the past key and value states
        past_key_values = DynamicCache()
        input_len = input_ids.shape[1]
        outputs = self.base_model(input_ids, past_key_values=past_key_values)
        new_token = 0

        for idx in range(max_new_tokens):
            input_id = outputs.logits[:, -1:].argmax(dim=-1)

            outputs = self.base_model(input_id, past_key_values=past_key_values)
            input_ids = torch.cat([input_ids, input_id], dim=-1)
            new_token += 1

            yield input_id

            # stop when following conditions are met
            if self.tokenizer.eos_token_id in input_ids[0, input_len:].tolist():
                break
            if new_token > max_new_tokens:
                break

    def _eagle_generate(self, 
                        input_ids: str, 
                        max_new_tokens: int, 
                        num_spec_steps: int = 3,
                        num_draft_tokens: int = 4,
                        ) -> str:
        padding = (torch.zeros(1, 1, dtype=torch.long) - 1).to(input_ids.device)
        past_key_values = DynamicCache()
        input_len = input_ids.shape[1]

        # TODO: support logits processor
        (
            draft_tokens, 
            retrieve_indices, 
            tree_mask, 
            tree_position_ids, 
            logits, 
            hidden_state, 
            sample_token
        ) = initialize_tree(
            input_ids=input_ids,
            base_model=self.base_model,
            draft_model=self.draft_model,
            past_key_values=past_key_values,
            logits_processor=None,
            num_spec_steps=num_spec_steps,
            num_draft_tokens=num_draft_tokens,
        )

        # tracking stats
        new_token = 0
        accepted_tokens = 0

        # generate response
        for idx in range(max_new_tokens):
            self.base_model.model.tree_mask = tree_mask
            draft_tokens = draft_tokens.to(input_ids.device)

            logits, hidden_state_new, outputs = tree_decoding(
                self,
                draft_tokens,
                past_key_values,
                tree_position_ids,
                input_ids,
                retrieve_indices,
            )

            draft_tokens = torch.cat((draft_tokens, padding), dim=1)
            candidates = draft_tokens[0, retrieve_indices]
            best_candidate, accept_length, sample_p = evaluate_posterior(
                logits, candidates, logits_processor
            )

            # do verification and update the output
            input_ids, draft_tokens, retrieve_indices, tree_mask, tree_position_ids, new_token, hidden_state, sample_token = update_inference_inputs(
                input_ids,
                candidates,
                best_candidate,
                accept_length,
                retrieve_indices,
                logits_processor,
                new_token,
                past_key_values_data,
                current_length_data,
                self,
                hidden_state_new,
                sample_p
            )

            yield input_ids

            if self.tokenizer.eos_token_id in input_ids[0, input_len:].tolist():
                break
            if new_token > max_new_tokens:
                break


class EagleTrainer(ABC):

    def step(self):
        pass

class OnlineEagleTrainer(EagleRunner):

    def step(self, input_ids, attention_mask) -> torch.Tensor:
        pass

class OfflineEagleTrainer(EagleTrainer):

    def step(self, hidden_states, attention_mask) -> torch.Tensor:
        pass