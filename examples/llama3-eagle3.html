
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Eagle3 for Llama3 &#8212; SGLang</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=a1637f0b"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=ccdb6887"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'examples/llama3-eagle3';</script>
    <link rel="icon" href="../_static/logo.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="‚Ü©Ô∏è Regenerate Datasets" href="../advanced_features/regenerate_dataset.html" />
    <link rel="prev" title="ü§ñ Benchmarking On SGLang" href="../basic_usage/benchmarking.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Nov 18, 2025"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="SGLang - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="SGLang - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started/install.html">üì¶ Installation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basic Usage</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/data_preparation.html">üìù Data Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/training.html">üöÄ Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/benchmarking.html">ü§ñ Benchmarking On SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Eagle3 for Llama3</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/regenerate_dataset.html">‚Ü©Ô∏è Regenerate Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/customization.html">üí° Customize Your Own Training</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/blob/main/examples/llama3-eagle3.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/edit/main/examples/llama3-eagle3.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/issues/new?title=Issue%20on%20page%20%2Fexamples/llama3-eagle3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/examples/llama3-eagle3.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Eagle3 for Llama3</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow">Workflow</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-prepare-environment">Step 1. Prepare environment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-prepare-model-dataset">Step 2. Prepare Model &amp; Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-start-training">Step 3. Start Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-benchmark">Step 4. benchmark</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="eagle3-for-llama3">
<h1>Eagle3 for Llama3<a class="headerlink" href="#eagle3-for-llama3" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>This document provides a step-by-step guide to reproducing the training process described in the EAGLE3 paper, using the script <code class="docutils literal notranslate"><span class="pre">examples/run_llama3_eagle3_sgl_online.sh</span></code>. We will walk through the script and explain each key step along the way.</p>
</section>
<section id="workflow">
<h2>Workflow<a class="headerlink" href="#workflow" title="Link to this heading">#</a></h2>
<section id="step-1-prepare-environment">
<h3>Step 1. Prepare environment<a class="headerlink" href="#step-1-prepare-environment" title="Link to this heading">#</a></h3>
<p>We suggest to use a virtual environment to make sure that all the dependencies can be correctly installed. If you want to use <code class="docutils literal notranslate"><span class="pre">python&gt;=3.12</span></code>, please set <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">SETUPTOOLS_USE_DISTUTILS=local</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>uv<span class="w"> </span>venv<span class="w"> </span>--python<span class="w"> </span><span class="m">3</span>.11
<span class="nb">source</span><span class="w"> </span>.venv/bin/activate
<span class="nb">cd</span><span class="w"> </span>PATH-TO-SpecForge
uv<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
uv<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-v<span class="w"> </span>.
</pre></div>
</div>
<p>After completing these steps, you can check if the installation is successful by running the following command. You should not see any error if the installation is successful.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import specforge&quot;</span>
</pre></div>
</div>
</section>
<section id="step-2-prepare-model-dataset">
<h3>Step 2. Prepare Model &amp; Dataset<a class="headerlink" href="#step-2-prepare-model-dataset" title="Link to this heading">#</a></h3>
<p>Next, we can start preparing the model and dataset. First, use these commands to download the model and the dataset.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>hf<span class="w"> </span>download<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct
hf<span class="w"> </span>download<span class="w"> </span>Aeala/ShareGPT_Vicuna_unfiltered<span class="w"> </span>--repo-type<span class="w"> </span>dataset
hf<span class="w"> </span>download<span class="w"> </span>HuggingFaceH4/ultrachat_200k<span class="w"> </span>--repo-type<span class="w"> </span>dataset

python<span class="w"> </span>scripts/prepare_data.py<span class="w"> </span>--dataset<span class="w"> </span>ultrachat<span class="w"> </span>--output_path<span class="w"> </span>/YOUR/PATH/Llama-3.1-8B-Instruct/dataset
python<span class="w"> </span>scripts/prepare_data.py<span class="w"> </span>--dataset<span class="w"> </span>sharegpt<span class="w"> </span>--output_path<span class="w"> </span>/YOUR/PATH/Llama-3.1-8B-Instruct/dataset
</pre></div>
</div>
<p>Then, launch the SGLang server and run <code class="docutils literal notranslate"><span class="pre">generate_data_by_target.py</span></code> to generate responses from the base model across different datasets. Make sure to update the <code class="docutils literal notranslate"><span class="pre">SYSTEM_PROMPT</span></code> value in <code class="docutils literal notranslate"><span class="pre">generate_data_by_target.py</span></code> to suit your requirements.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span>i<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="o">{</span><span class="m">1</span>..4<span class="o">}</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">    </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="nv">$i</span><span class="w"> </span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--cuda-graph-bs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="m">256</span><span class="w"> </span><span class="m">512</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dtype<span class="w"> </span>bfloat16<span class="w"> </span>--mem-frac<span class="o">=</span><span class="m">0</span>.8<span class="w"> </span>--port<span class="w"> </span><span class="k">$((</span><span class="m">30000</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">i</span><span class="k">))</span><span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>

python<span class="w"> </span>scripts/generate_data_by_target.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model-name<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--raw-data-file<span class="w"> </span>/YOUR/PATH/Llama-3.1-8B-Instruct/dataset/sharegpt.jsonl<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output-dir<span class="w"> </span>/YOUR/PATH/Llama-3.1-8B-Instruct/generated-dataset/sharegpt-llama-3.1-8b-instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max-concurrency<span class="w"> </span><span class="m">512</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num-per-shard<span class="w"> </span><span class="m">50000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--server-address-port<span class="w"> </span><span class="m">127</span>.0.0.1:30001<span class="w"> </span><span class="m">127</span>.0.0.1:30002<span class="w"> </span><span class="m">127</span>.0.0.1:30003<span class="w"> </span><span class="m">127</span>.0.0.1:30004

python<span class="w"> </span>scripts/generate_data_by_target.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model-name<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--raw-data-file<span class="w"> </span>/YOUR/PATH/Llama-3.1-8B-Instruct/dataset/ultrachat.jsonl<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output-dir<span class="w"> </span>/YOUR/PATH/Llama-3.1-8B-Instruct/generated-dataset/ultrachat-llama-3.1-8b-instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max-concurrency<span class="w"> </span><span class="m">512</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num-per-shard<span class="w"> </span><span class="m">50000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--server-address-port<span class="w"> </span><span class="m">127</span>.0.0.1:30001<span class="w"> </span><span class="m">127</span>.0.0.1:30002<span class="w"> </span><span class="m">127</span>.0.0.1:30003<span class="w"> </span><span class="m">127</span>.0.0.1:30004
</pre></div>
</div>
<p>After completing these steps, you can review the error entries in <code class="docutils literal notranslate"><span class="pre">error.jsonl</span></code>. Most of them will likely be <code class="docutils literal notranslate"><span class="pre">request</span> <span class="pre">timeout</span></code>. You can then decide whether you want to regenerate those samples. In my case, I chose not to, so I simply deleted error.jsonl before uploading to Hugging Face. The following command is used:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>hf<span class="w"> </span>repo<span class="w"> </span>create<span class="w"> </span>zhuyksir/Ultrachat-Sharegpt-Llama3.1-8B<span class="w"> </span>--type<span class="w"> </span>dataset
hf<span class="w"> </span>upload<span class="w"> </span>/YOUR/PATH/Llama-3.1-8B-Instruct/generated-dataset/ultrachat-llama-3.1-8b-instruct<span class="w"> </span>--commit-message<span class="w"> </span><span class="s2">&quot;generated dataset by Llama3.1-8B&quot;</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;zhuyksir/Ultrachat-Sharegpt-Llama3.1-8B&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">ds</span><span class="o">.</span><span class="n">to_json</span><span class="p">(</span><span class="s2">&quot;merged.jsonl&quot;</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;records&quot;</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
<span class="n">test_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>Alternatively, For <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.1-8B-Instruct</span></code>, you can use the dataset we generated: <a class="reference external" href="https://huggingface.co/datasets/zhuyksir/Ultrachat-Sharegpt-Llama3.1-8B">zhuyksir/Ultrachat-Sharegpt-Llama3.1-8B</a>.</p>
<p>Each row should have this structure:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="err">XXX</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;conversations&quot;</span><span class="p">:[</span>
<span class="w">        </span><span class="p">{</span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;system&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="err">XXX</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="err">XXX</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="err">XXX</span><span class="p">},</span>
<span class="w">        </span><span class="err">...</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Second, we need to pre-build the cache for training.</p>
<ul class="simple">
<li><p>During training, the text must be encoded into input IDs. These encoding steps can be performed before training begins. The resulting cache file will be saved under <code class="docutils literal notranslate"><span class="pre">$CACHE_DIR</span></code>.</p></li>
<li><p>The script also selects the vocabulary with the top-k size.</p></li>
<li><p>With the option <code class="docutils literal notranslate"><span class="pre">--view</span> <span class="pre">train-data</span></code>, you can inspect the dataset by index (e.g., index 1 or index 2 in the example below). This helps verify that the loss mask is generated correctly:</p>
<ul>
<li><p>Green text indicates tokens where <code class="docutils literal notranslate"><span class="pre">loss_mask</span> <span class="pre">==</span> <span class="pre">1</span></code>.</p></li>
<li><p>Red text indicates tokens where <code class="docutils literal notranslate"><span class="pre">loss_mask</span> <span class="pre">==</span> <span class="pre">0</span> <span class="pre">(typically</span> <span class="pre">user</span> <span class="pre">input</span> <span class="pre">and</span> <span class="pre">system</span> <span class="pre">prompt)</span></code>. Since the goal is to train the draft model only on the target model‚Äôs output, user text must be masked out. In other words, only tokens generated by the target model should contribute to the loss.</p></li>
</ul>
</li>
<li><p>You might see this warning. <code class="docutils literal notranslate"><span class="pre">WARNING:</span> <span class="pre">No</span> <span class="pre">assistant</span> <span class="pre">response</span> <span class="pre">spans</span> <span class="pre">found</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">conversation</span> <span class="pre">text.</span></code>This occurs when, during data generation, an error causes a sample to contain only user inputs without any assistant responses. You can safely ignore this warning‚Äîthe loss mask for such samples is set entirely to zero.</p></li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>scripts/build_eagle3_dataset_cache.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--target-model-path<span class="w"> </span><span class="nv">$MODEL_PATH</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--draft-model-config<span class="w"> </span>./configs/llama3-8B-eagle3.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--train-data-path<span class="w"> </span><span class="nv">$DATASET_PATH</span>/sharegpt_ultrachat_train.jsonl<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval-data-path<span class="w"> </span><span class="nv">$DATASET_PATH</span>/sharegpt_ultrachat_test.jsonl<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cache-dir<span class="w"> </span><span class="nv">$CACHE_DIR</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--chat-template<span class="w"> </span><span class="nv">$CHAT_TEMPLATE</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max-length<span class="w"> </span><span class="nv">$MAX_LENGTH</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--view-train-data<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span>
</pre></div>
</div>
</section>
<section id="step-3-start-training">
<h3>Step 3. Start Training<a class="headerlink" href="#step-3-start-training" title="Link to this heading">#</a></h3>
<p>Use the following script to train.</p>
<ul class="simple">
<li><p>set <code class="docutils literal notranslate"><span class="pre">total-steps=800000,</span> <span class="pre">learning-rate=5e-5</span></code> to align with <a class="reference external" href="https://github.com/SafeAILab/EAGLE/blob/main/eagle/traineagle3/ds_config.json">EAGLE official repo config</a>. Feel Free to change this settings to do your own experiments. <code class="docutils literal notranslate"><span class="pre">total-steps</span></code> and <code class="docutils literal notranslate"><span class="pre">warmup-ratio</span></code> decide the increasement curve of learning rate.</p></li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NUM_GPUS</span><span class="o">=</span><span class="m">4</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OUTPUT_DIR</span><span class="o">=</span>/YOUR/PATH/Llama-3.1-8B-Instruct/dev_outputs/
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">4</span>,5,6,7<span class="w"> </span>torchrun<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--standalone<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--nproc_per_node<span class="w"> </span><span class="nv">$NUM_GPUS</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>scripts/train_eagle3_sgl_online.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--target-model-path<span class="w"> </span><span class="nv">$MODEL_PATH</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model-path<span class="w"> </span><span class="nv">$MODEL_PATH</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--draft-model-config<span class="w"> </span>./configs/llama3-8B-eagle3.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--train-data-path<span class="w"> </span><span class="nv">$DATASET_PATH</span>/sharegpt_ultrachat_train.jsonl<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval-data-path<span class="w"> </span><span class="nv">$DATASET_PATH</span>/sharegpt_ultrachat_test.jsonl<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tp-size<span class="w"> </span><span class="nv">$NUM_GPUS</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output-dir<span class="w"> </span><span class="nv">$OUTPUT_DIR</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num-epochs<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--batch-size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning-rate<span class="w"> </span>5e-5<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--draft-attention-backend<span class="w"> </span>flex_attention<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max-length<span class="w"> </span><span class="nv">$MAX_LENGTH</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--chat-template<span class="w"> </span><span class="nv">$CHAT_TEMPLATE</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cache-dir<span class="w"> </span><span class="nv">$CACHE_DIR</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mem-frac<span class="o">=</span><span class="m">0</span>.4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--total-steps<span class="o">=</span><span class="m">800000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dist-timeout<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--wandb-project<span class="w"> </span>llama3-8b-eagle3<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--wandb-name<span class="w"> </span>sgl-online<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--report-to<span class="w"> </span>wandb
</pre></div>
</div>
</section>
<section id="step-4-benchmark">
<h3>Step 4. benchmark<a class="headerlink" href="#step-4-benchmark" title="Link to this heading">#</a></h3>
<p>For <code class="docutils literal notranslate"><span class="pre">Llama3.1-8B</span></code>, we add a system prompt to all training data, following the approach used in the official repository. Consequently, when benchmarking, we should also include this system prompt to obtain the full accept length. Please uncomment the corresponding line and add the system prompt.</p>
<p>The four numbers in the config represent: <code class="docutils literal notranslate"><span class="pre">batch_size,</span> <span class="pre">num_steps,</span> <span class="pre">topk,</span> <span class="pre">num_verify_tokens</span></code>.  You can adjust the values in the config list to experiment with different test cases.</p>
<p>I have upload my trained eagle model in <a class="reference external" href="https://huggingface.co/zhuyksir/EAGLE3-Llama-3.1-8B-Instruct">zhuyksir/EAGLE3-Llama-3.1-8B-Instruct</a>. You are welcome to download and check its accept length.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">config_list</span><span class="o">=(</span>
<span class="w">    </span><span class="s2">&quot;4,3,1,4&quot;</span>
<span class="w">    </span><span class="s2">&quot;4,7,10,60&quot;</span>
<span class="o">)</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">4</span>,5,6,7<span class="w"> </span>python3<span class="w"> </span>bench_model_speedup.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model-path<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-draft-model-path<span class="w"> </span>/YOUR/PATH/Llama-3.1-8B-Instruct/dev_outputs/epoch_0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="m">20001</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tp-size<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--config-list<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">config_list</span><span class="p">[@]</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--benchmark-list<span class="w"> </span>mtbench:80<span class="w"> </span>gsm8k:200<span class="w"> </span>humaneval:200<span class="w"> </span>math500:200<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output<span class="w"> </span>output.jsonl
</pre></div>
</div>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../basic_usage/benchmarking.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">ü§ñ Benchmarking On SGLang</p>
      </div>
    </a>
    <a class="right-next"
       href="../advanced_features/regenerate_dataset.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">‚Ü©Ô∏è Regenerate Datasets</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow">Workflow</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-prepare-environment">Step 1. Prepare environment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-prepare-model-dataset">Step 2. Prepare Model &amp; Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-start-training">Step 3. Start Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-benchmark">Step 4. benchmark</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By SpecForge Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025-2025, SpecForge.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Nov 18, 2025.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>