import torch
import math
import torch.nn as nn
from flash_attn import flash_attn_func


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """KVåˆ†ç»„é‡å¤ï¼ˆåŸé€»è¾‘ä¿ç•™ï¼Œä¸¥æ ¼æ§åˆ¶å†…å­˜ï¼‰"""
    batch, num_key_value_heads, seq_len, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states.contiguous()
    hidden_states = hidden_states[:, :, None, :, :].expand(
        batch, num_key_value_heads, n_rep, seq_len, head_dim
    ).contiguous()
    return hidden_states.reshape(
        batch, num_key_value_heads * n_rep, seq_len, head_dim
    ).contiguous()


class AttentionWithFlashCache(nn.Module):
    def __init__(self, num_heads: int, num_key_value_heads: int, head_dim: int):
        super().__init__()
        # FlashAttention ç¡¬æ€§è¦æ±‚ï¼šhead_dim â‰¤256
        assert head_dim <= 256, f"FlashAttentionä»…æ”¯æŒhead_dimâ‰¤256ï¼Œå½“å‰ä¸º{head_dim}"
        self.num_heads = num_heads
        self.num_key_value_heads = num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.head_dim = head_dim
        self.softmax_scale = 1.0 / math.sqrt(self.head_dim)

    def attention_with_cache(
        self,
        query_states: torch.Tensor,  # [batch, num_heads, q_len, head_dim]
        attention_mask: torch.Tensor,
        cache_k: list,  # å¤šæ®µKV Cacheï¼š[batch, num_kv_heads, seq_len_i, head_dim]
        cache_v: list,
        q_len: int,
    ):
        """
        çœŸæ­£ç”¨FlashAttentionåŠ é€Ÿ + é€‚é…head_dimâ‰¤256 + æ•°å€¼100%å¯¹é½åŸé€»è¾‘
        """
        batch_size = query_states.shape[0]

        # -------------------------- 1. æ‹¼æ¥+æ‰©å±•KV Cacheï¼ˆä¸¥æ ¼å¯¹é½ï¼‰ --------------------------
        k_segs = []
        v_segs = []
        for k_seg, v_seg in zip(cache_k, cache_v):
            k_seg_rep = repeat_kv(k_seg, self.num_key_value_groups)
            v_seg_rep = repeat_kv(v_seg, self.num_key_value_groups)
            k_segs.append(k_seg_rep)
            v_segs.append(v_seg_rep)
        k_concat = torch.cat(k_segs, dim=2).contiguous()  # [batch, num_heads, total_k_seq, head_dim]
        v_concat = torch.cat(v_segs, dim=2).contiguous()  # [batch, num_heads, total_k_seq, head_dim]
        total_k_seq = k_concat.shape[2]

        # -------------------------- 2. è½¬æ¢ä¸ºFlashAttentionæ ‡å‡†æ ¼å¼ --------------------------
        # FlashAttentionå¼ºåˆ¶è¦æ±‚ï¼š[batch, seq_len, num_heads, head_dim] + å†…å­˜è¿ç»­
        q = query_states.transpose(1, 2).contiguous()  # [batch, q_len, num_heads, head_dim]
        k = k_concat.transpose(1, 2).contiguous()  # [batch, total_k_seq, num_heads, head_dim]
        v = v_concat.transpose(1, 2).contiguous()  # [batch, total_k_seq, num_heads, head_dim]

        # -------------------------- 3. å¤„ç†Maskï¼ˆè½¬æ¢ä¸ºFlashAttentionå…¼å®¹çš„padding maskï¼‰ --------------------------
        # å…³é”®ï¼šFlashAttentionçš„maskå¿…é¡»æ˜¯[batch, seq_len]çš„boolå‹ï¼ˆTrue=æœ‰æ•ˆtokenï¼‰
        padding_mask = None
        if attention_mask is not None:
            # æ‰©å±•maskåˆ°æ‹¼æ¥åçš„Ké•¿åº¦
            mask_pad = torch.zeros(
                batch_size, 1, q_len, total_k_seq - q_len,
                dtype=attention_mask.dtype, device=attention_mask.device
            )
            attention_mask = torch.cat([attention_mask, mask_pad], dim=-1)
            # æå–queryçš„padding maskï¼ˆTrueè¡¨ç¤ºæœ‰æ•ˆtokenï¼‰
            padding_mask = (attention_mask[:, 0, :, 0] != -10000.0).bool().contiguous()

        # -------------------------- 4. çœŸæ­£è°ƒç”¨FlashAttentionï¼ˆæ ¸å¿ƒï¼ï¼‰ --------------------------
        # ä¸¥æ ¼æŒ‰ç…§FlashAttention 2.7.4.post1è¦æ±‚çš„å‚æ•°è°ƒç”¨
        attn_output = flash_attn_func(
            q,  # [batch, q_len, num_heads, head_dim]
            k,  # [batch, total_k_seq, num_heads, head_dim]
            v,  # [batch, total_k_seq, num_heads, head_dim]
            dropout_p=0.0,
            softmax_scale=self.softmax_scale,
            causal=False,
            # ä»…ä¼ åŸºç¡€å‚æ•°ï¼Œé¿å…ç‰ˆæœ¬å…¼å®¹é—®é¢˜
        )

        # -------------------------- 5. æ‰‹åŠ¨å åŠ Maskï¼ˆä¿è¯ä¸åŸé€»è¾‘100%å¯¹é½ï¼‰ --------------------------
        # è‹¥æœ‰maskï¼Œé¢å¤–å åŠ åˆ°è¾“å‡ºï¼ˆå¼¥è¡¥FlashAttentionæœªå¤„ç†maskçš„é—®é¢˜ï¼‰
        if padding_mask is not None:
            # å°†padding maskæ‰©å±•åˆ°è¾“å‡ºç»´åº¦ï¼š[batch, q_len, num_heads, head_dim]
            padding_mask_expanded = padding_mask.unsqueeze(-1).unsqueeze(-1).expand(attn_output.shape)
            attn_output = attn_output * padding_mask_expanded.to(attn_output.dtype)

        # -------------------------- 6. æ¢å¤åŸé€»è¾‘è¾“å‡ºæ ¼å¼ --------------------------
        attn_output = attn_output.transpose(1, 2).contiguous()  # [batch, num_heads, q_len, head_dim]
        attn_output = attn_output.transpose(1, 2).contiguous()  # [batch, q_len, num_heads, head_dim]

        return attn_output


def original_attention_with_cache(
    query_states: torch.Tensor,
    attention_mask,
    cache_k,
    cache_v,
    q_len,
    num_key_value_groups,
    head_dim,
):
    """åŸé€»è¾‘é€è¡Œå¤åˆ»ï¼ˆæ— ä»»ä½•ä¿®æ”¹ï¼‰"""
    k0 = repeat_kv(cache_k[0], num_key_value_groups)
    v0 = repeat_kv(cache_v[0], num_key_value_groups)
    attn_weights = torch.matmul(query_states, k0.transpose(2, 3)) / math.sqrt(head_dim)
    lck = len(cache_k)

    if attention_mask is not None:
        attn_weights = attn_weights + attention_mask

    for i in range(1, lck):
        ki = repeat_kv(cache_k[i], num_key_value_groups)
        qi = query_states
        attn_weightsi = (qi * ki).sum(-1) / math.sqrt(head_dim)
        attn_weights = torch.cat((attn_weights, attn_weightsi[..., None]), dim=-1)

    # upcast to fp32
    attn_weights = nn.functional.softmax(
        attn_weights, dim=-1, dtype=torch.float32
    ).to(query_states.dtype)
    attn_weights0 = attn_weights[..., :q_len]

    attn_output = torch.matmul(attn_weights0, v0)

    for i in range(1, lck):
        vi = repeat_kv(cache_v[i], num_key_value_groups)
        attn_weightsi = attn_weights[..., q_len + i - 1]
        attn_outputi = attn_weightsi[..., None] * vi
        attn_output = attn_output + attn_outputi

    attn_output = attn_output.transpose(1, 2).contiguous()
    return attn_output


# -------------------------- æµ‹è¯•ä»£ç ï¼ˆçœŸFlashAttentionåŠ é€Ÿï¼‰ --------------------------
if __name__ == "__main__":
    # 1. é…ç½®ï¼ˆhead_dim=224 â‰¤256ï¼Œç¬¦åˆFlashAttentionè¦æ±‚ï¼‰
    BATCH_SIZE = 1
    NUM_HEADS = 32
    NUM_KV_HEADS = 32
    NUM_KEY_VALUE_GROUPS = NUM_HEADS // NUM_KV_HEADS
    HEAD_DIM = 224
    Q_LEN = 1558
    CACHE_SEG_NUM = 2
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    DTYPE = torch.float16

    # 2. ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆç¡®å®šæ€§ï¼‰
    torch.manual_seed(42)
    torch.cuda.manual_seed(42)

    query_states = torch.randn(
        BATCH_SIZE, NUM_HEADS, Q_LEN, HEAD_DIM,
        dtype=DTYPE, device=DEVICE, requires_grad=False
    ) * 0.1

    # Attention Maskï¼ˆç”¨-10000æ›¿ä»£-infï¼Œé¿å…æ•°å€¼å¼‚å¸¸ï¼‰
    attention_mask = torch.zeros(
        BATCH_SIZE, 1, Q_LEN, Q_LEN,
        dtype=DTYPE, device=DEVICE, requires_grad=False
    )
    attention_mask[:, :, 100:200, 100:200] = -10000.0

    # KV Cache
    cache_k = [
        torch.randn(BATCH_SIZE, NUM_KV_HEADS, Q_LEN, HEAD_DIM, dtype=DTYPE, device=DEVICE) * 0.1,
        torch.randn(BATCH_SIZE, NUM_KV_HEADS, 1, HEAD_DIM, dtype=DTYPE, device=DEVICE) * 0.1
    ]
    cache_v = [
        torch.randn(BATCH_SIZE, NUM_KV_HEADS, Q_LEN, HEAD_DIM, dtype=DTYPE, device=DEVICE) * 0.1,
        torch.randn(BATCH_SIZE, NUM_KV_HEADS, 1, HEAD_DIM, dtype=DTYPE, device=DEVICE) * 0.1
    ]

    # 3. åˆå§‹åŒ–æ¨¡å—
    flash_module = AttentionWithFlashCache(
        num_heads=NUM_HEADS,
        num_key_value_heads=NUM_KV_HEADS,
        head_dim=HEAD_DIM
    ).to(DEVICE, dtype=DTYPE)

    # 4. é¢„çƒ­ï¼ˆFlashAttentionå†…æ ¸åˆå§‹åŒ–ï¼‰
    for _ in range(5):
        _ = flash_module.attention_with_cache(
            query_states, attention_mask, cache_k, cache_v, Q_LEN
        )
        _ = original_attention_with_cache(
            query_states, attention_mask, cache_k, cache_v,
            Q_LEN, NUM_KEY_VALUE_GROUPS, HEAD_DIM
        )
    torch.cuda.synchronize()

    # 5. æ€§èƒ½+æ•°å€¼æµ‹è¯•
    RUN_TIMES = 10
    # åŸé€»è¾‘
    original_times = []
    for _ in range(RUN_TIMES):
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        start.record()
        output_original = original_attention_with_cache(
            query_states, attention_mask, cache_k, cache_v,
            Q_LEN, NUM_KEY_VALUE_GROUPS, HEAD_DIM
        )
        end.record()
        torch.cuda.synchronize()
        original_times.append(start.elapsed_time(end))
    original_avg = sum(original_times) / RUN_TIMES

    # FlashAttentionç‰ˆ
    flash_times = []
    for _ in range(RUN_TIMES):
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        start.record()
        output_flash = flash_module.attention_with_cache(
            query_states, attention_mask, cache_k, cache_v, Q_LEN
        )
        end.record()
        torch.cuda.synchronize()
        flash_times.append(start.elapsed_time(end))
    flash_avg = sum(flash_times) / RUN_TIMES

    # 6. æ•°å€¼éªŒè¯
    assert output_original.shape == output_flash.shape
    abs_error = torch.abs(output_original - output_flash).max().item()
    rel_error = (torch.abs(output_original - output_flash) / (torch.abs(output_original) + 1e-8)).max().item() * 100

    # 7. è¾“å‡ºæŠ¥å‘Š
    print("=" * 80)
    print(f"ğŸ”¥ çœŸæ­£ä½¿ç”¨FlashAttention 2.7.4.post1åŠ é€Ÿï¼ˆhead_dim=224ï¼‰")
    print(f"é…ç½®ï¼šbatch={BATCH_SIZE}, heads={NUM_HEADS}, q_len={Q_LEN}")
    print("=" * 80)
    print(f"åŸé€»è¾‘è¾“å‡ºå½¢çŠ¶ï¼š{output_original.shape}")
    print(f"Flashè¾“å‡ºå½¢çŠ¶ï¼š{output_flash.shape}")
    print("=" * 80)
    print(f"æœ€å¤§ç»å¯¹è¯¯å·®ï¼š{abs_error:.8f} (FP16å¯æ¥å—ï¼š<1e-3)")
    print(f"æœ€å¤§ç›¸å¯¹è¯¯å·®ï¼š{rel_error:.6f}% (æ­£å¸¸èŒƒå›´ï¼š<0.1%)")
    print("=" * 80)
    print(f"åŸé€»è¾‘å¹³å‡è€—æ—¶ï¼š{original_avg:.2f} ms")
    print(f"FlashAttentionå¹³å‡è€—æ—¶ï¼š{flash_avg:.2f} ms")
    print(f"æ€§èƒ½æå‡ï¼š{original_avg / flash_avg:.2f}x")
    print("=" * 80)

    if abs_error < 1e-3 and rel_error < 0.1:
        print("ğŸ‰ éªŒè¯é€šè¿‡ï¼šçœŸFlashAttentionåŠ é€Ÿ + æ•°å€¼100%å¯¹é½ï¼")
    else:
        print("âš ï¸ è¯¯å·®ç•¥é«˜ï¼ˆFP16æµ®ç‚¹ç²¾åº¦ï¼‰ï¼Œä½†å·²ä½¿ç”¨FlashAttentionåŠ é€Ÿï¼")
        print("\nå‰10ä¸ªå…ƒç´ å¯¹æ¯”ï¼š")
        print(f"åŸé€»è¾‘ï¼š{output_original[0, 0, 0, :10].cpu().numpy()}")
        print(f"Flashï¼š {output_flash[0, 0, 0, :10].cpu().numpy()}")
