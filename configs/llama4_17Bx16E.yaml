base: ./common.yaml

data:
  train:
    path: /sgl-workspace/sharegpt-eagle
    batch_size: 1

model:
  target:
    pretrained_model_name_or_path: /tmp/Llama-4-Scout-17B-16E-Instruct
  draft:
    type: LlamaForCausalLMEagle3
    attention_bias: false
    attention_dropout: 0.0
    bos_token_id: null
    draft_vocab_size: 202048
    eos_token_id: null
    head_dim: 128
    hidden_act: "silu"
    hidden_size: 5120
    initializer_range: 0.02
    intermediate_size: 32768
    max_position_embeddings: 1048576
    mlp_bias: false
    model_type: "llama"
    num_attention_heads: 40
    num_hidden_layers: 1
    num_key_value_heads: 8
    pretraining_tp: 1
    rms_norm_eps: 1e-05
    rope_scaling:
      factor: 8.0
      high_freq_factor: 4.0
      low_freq_factor: 1.0
      original_max_position_embeddings: 8192
      rope_type: "llama3"
    rope_theta: 500000.0
    tie_word_embeddings: false
    torch_dtype: "bfloat16"
    use_cache: true
    vocab_size: 202048
